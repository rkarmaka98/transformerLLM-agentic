{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a106b688",
   "metadata": {},
   "source": [
    "# Testing LangChain and LangSmith on LLM Models\n",
    "\n",
    "This notebook documents the process of integrating and testing LangChain and LangSmith with large language models (LLMs). We demonstrate how to load the necessary environment configurations, connect with APIs, and perform model interactions while tracing execution using LangSmith. The workflow involves setting up API keys, configuring environment variables, and running test cases to ensure end-to-end functionality with our LLM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab361c5",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac54071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Set up LangSmith environment variables if not already set\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your LangSmith API key (optional): \"\n",
    "    )\n",
    "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
    "        prompt='Enter your LangSmith Project Name (default = \"default\"): '\n",
    "    )\n",
    "    if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
    "        os.environ[\"LANGSMITH_PROJECT\"] = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b22a443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9635ca2af8540fd814c6c25f5010bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch, time, gc\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "max_memory = {0: \"5GiB\", \"cpu\": \"25GiB\"}\n",
    "\n",
    "tok_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "model_id = tok_id\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    quantization_config=bnb_config,\n",
    "    max_memory=max_memory,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Safety: some Qwen-family tokenizers miss pad id; generation needs it\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f4604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Iterator\n",
    "from langchain_core.runnables import Runnable, RunnableLambda, RunnableGenerator\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langsmith import traceable, get_current_run_tree\n",
    "\n",
    "\n",
    "def _apply_chat_template(messages: List[Dict[str, str]], max_prompt_tokens: int = 512):\n",
    "    \"\"\"Your exact formatting path: messages -> chat template -> tokenized tensors (on correct device).\"\"\"\n",
    "    prompt_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_prompt_tokens,\n",
    "    )\n",
    "    device = next(model.parameters()).device\n",
    "    return {k: v.to(device) for k, v in prompt_inputs.items()}\n",
    "\n",
    "\n",
    "def _decode_new_tokens(generated_ids, input_len):\n",
    "    return tokenizer.decode(generated_ids[0][input_len:], skip_special_tokens=False)\n",
    "\n",
    "\n",
    "@traceable(name=\"deepseek_r1_generate\")  # LangSmith span\n",
    "def _generate_once(\n",
    "    messages: List[Dict[str, str]],\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.95,\n",
    "    **gen_kwargs,\n",
    ") -> str:\n",
    "    \"\"\"Single-shot generate with your timing + CUDA sync, traced in LangSmith.\"\"\"\n",
    "    inputs = _apply_chat_template(messages)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # Collect basic perf stats\n",
    "    gen_len = int(outputs.shape[-1] - inputs[\"input_ids\"].shape[-1])\n",
    "    toks_per_s = gen_len / dt if dt > 0 else float(\"inf\")\n",
    "    vram_alloc = (\n",
    "        torch.cuda.memory_allocated() / 1e6 if torch.cuda.is_available() else 0.0\n",
    "    )\n",
    "    vram_reserved = (\n",
    "        torch.cuda.memory_reserved() / 1e6 if torch.cuda.is_available() else 0.0\n",
    "    )\n",
    "\n",
    "    # `traceable` automatically creates the span; we can add metadata via return value\n",
    "    text = _decode_new_tokens(outputs, inputs[\"input_ids\"].shape[-1])\n",
    "\n",
    "    # Add metadata directly to this run\n",
    "    rt = get_current_run_tree()\n",
    "    if rt:\n",
    "        rt.metadata.update(\n",
    "            {\n",
    "                \"model\": \"DeepSeek-R1-Distill-Qwen-7B\",\n",
    "                \"quant\": \"4bit-nf4\",\n",
    "                \"device\": str(next(model.parameters()).device),\n",
    "            }\n",
    "        )\n",
    "        rt.set(\n",
    "            usage_metadata={\n",
    "                \"output_tokens\": outputs.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"metrics\": {\n",
    "            \"latency_s\": round(dt, 4),\n",
    "            \"gen_tokens\": gen_len,\n",
    "            \"tokens_per_s\": round(toks_per_s, 2),\n",
    "            \"vram_alloc_mb\": round(vram_alloc, 1),\n",
    "            \"vram_reserved_mb\": round(vram_reserved, 1),\n",
    "        },\n",
    "        \"gen_params\": {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            **{\n",
    "                k: v\n",
    "                for k, v in gen_kwargs.items()\n",
    "                if k in (\"repetition_penalty\", \"top_k\")\n",
    "            },\n",
    "        },\n",
    "        \"device_map\": str(getattr(model, \"hf_device_map\", \"unknown\")),\n",
    "        \"dtype\": str(getattr(model.config, \"torch_dtype\", \"auto\")),\n",
    "    }\n",
    "\n",
    "\n",
    "# LangChain Runnable (non-streaming)\n",
    "lc_generate: Runnable[List[Dict[str, str]], Dict[str, Any]] = RunnableLambda(\n",
    "    _generate_once\n",
    ")\n",
    "\n",
    "\n",
    "# Streaming variant using Transformersâ€™ TextStreamer-like incremental decode\n",
    "def _stream_tokens(\n",
    "    messages: List[Dict[str, str]],\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.95,\n",
    "    **gen_kwargs,\n",
    ") -> Iterator[str]:\n",
    "    inputs = _apply_chat_template(messages)\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    # Greedy-ish incremental loop using `model.generate` streaming via `stopping_criteria` is messy;\n",
    "    # simplest is `generate` then chunk decode. For true token-level streaming, prefer HF TextStreamer.\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    decoded = _decode_new_tokens(outputs, input_len)\n",
    "    # yield in chunks so LangChain callbacks still get pieces\n",
    "    chunk_sz = 40\n",
    "    for i in range(0, len(decoded), chunk_sz):\n",
    "        yield decoded[i : i + chunk_sz]\n",
    "\n",
    "\n",
    "lc_stream: Runnable[List[Dict[str, str]], Iterator[str]] = RunnableGenerator(\n",
    "    _stream_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22130523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out the key differences between supervised and unsupervised learning in machine learning. Hmm, I remember from my studies that these are two main types of machine learning, but I'm a bit fuzzy on the details. Let me try to break it down.\n",
      "\n",
      "First, I think supervised learning involves using labeled data. That means the data has the correct answers or outputs already provided. So, the model is trained on this data to predict the outcomes. For example, if I'm trying to predict whether an email is spam or not, I would have a dataset where each email is labeled as spam or not spam.\n",
      "{'latency_s': 11.0349, 'gen_tokens': 128, 'tokens_per_s': 11.6, 'vram_alloc_mb': 5588.1, 'vram_reserved_mb': 5943.3}\n",
      "Okay, so I need to figure out how to solve this problem. Hmm, let me start by understanding what the problem is asking. It says, \"Solve the equation 2x + 3 = 7.\" Alright, that seems straightforward. I've done similar problems before, but I want to make sure I approach it correctly.\n",
      "\n",
      "First, I remember that solving an equation means finding the value of the variable that makes the equation true. In this case, the variable is x. So, I need to isolate x on one side of the equation. The equation is 2x + 3 = 7. To isolate"
     ]
    }
   ],
   "source": [
    "# Your original messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Imagine you are asked to consider the following question: What are the key differences between supervised and unsupervised learning in machine learning? Please provide a detailed explanation.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Single response\n",
    "result = lc_generate.invoke(messages)\n",
    "print(result[\"text\"])\n",
    "print(result[\"metrics\"])  # latency, toks/s, VRAM, etc.\n",
    "\n",
    "# Streaming response\n",
    "for chunk in lc_stream.stream(messages):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bafc86fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to summarize the Chinese Room critique in six lines. Hmm, I remember the Chinese Room is a thought experiment by John Searle, right? It's about whether a machine can truly understand or have consciousness. Let me think about how it works.\n",
      "\n",
      "So, the setup is a room with a set of physical symbols, like cards, and a person who follows rules to manipulate these symbols. The person doesn't understand the meaning behind the symbols, they just do the tasks. From the outside, it looks like the machine is understanding and processing information, but really, it's just following procedures.\n",
      "\n",
      "The critique, I\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a precise reasoning assistant.\"), (\"human\", \"{question}\")]\n",
    ")\n",
    "\n",
    "\n",
    "# Turn LC messages -> your HF messages dict format\n",
    "def to_hf_messages(lc_messages) -> List[Dict[str, str]]:\n",
    "    out = []\n",
    "    for m in lc_messages:\n",
    "        if m.type == \"human\":\n",
    "            out.append({\"role\": \"user\", \"content\": m.content})\n",
    "        elif m.type == \"system\":\n",
    "            out.append({\"role\": \"system\", \"content\": m.content})\n",
    "        elif m.type == \"ai\":\n",
    "            out.append({\"role\": \"assistant\", \"content\": m.content})\n",
    "    return out\n",
    "\n",
    "\n",
    "chain = (\n",
    "    prompt\n",
    "    | (lambda vars: to_hf_messages(vars.to_messages()))  # LCEL lambda\n",
    "    | lc_generate\n",
    ")\n",
    "\n",
    "res = chain.invoke({\"question\": \"Summarize the Chinese Room critique in 6 lines.\"})\n",
    "print(res[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc0907da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_token_count(messages):\n",
    "    enc = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,  # <â€” important\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    ids = enc[\"input_ids\"]\n",
    "    # handle (seq,) or (1, seq)\n",
    "    if ids.dim() == 1:\n",
    "        return int(ids.size(0))\n",
    "    elif ids.dim() == 2:\n",
    "        return int(ids.size(1))\n",
    "    else:\n",
    "        return int(ids.numel())\n",
    "\n",
    "\n",
    "prompt_messages = messages  # from above\n",
    "result = lc_generate.invoke(\n",
    "    prompt_messages,\n",
    "    config={\n",
    "        \"metadata\": {\n",
    "            \"model\": model_id,\n",
    "            \"quant\": \"4bit-nf4\",\n",
    "            \"attn\": \"sdpa\",\n",
    "            \"device\": str(next(model.parameters()).device),\n",
    "            \"max_memory\": {str(k): v for k, v in max_memory.items()},  # JSON-safe\n",
    "            \"prompt_tokens\": prompt_token_count(prompt_messages),\n",
    "        },\n",
    "        \"tags\": [\"exp\", \"deepseek-r1\", \"local-hf\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dcb35d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out the key differences between supervised and unsupervised learning. Hmm, I remember from my studies that both are types of machine learning, but I'm a bit fuzzy on the exact distinctions. Let me try to break this down.\n",
      "\n",
      "First, I think supervised learning involves using labeled data. That means the data has known outcomes or answers. Like, if I'm trying to predict whether an email is spam or not, each email would have a label: spam or not spam. The model learns from these labeled examples to make predictions on new, unseen data. So, the key here is that the data has\n",
      "Perf: {'latency_s': 11.2741, 'gen_tokens': 128, 'tokens_per_s': 11.35, 'vram_alloc_mb': 5588.1, 'vram_reserved_mb': 5943.3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3237"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = lc_generate.invoke(messages)\n",
    "print(res[\"text\"])\n",
    "print(\"Perf:\", res[\"metrics\"])\n",
    "\n",
    "# Your existing cleanup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "del res\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
