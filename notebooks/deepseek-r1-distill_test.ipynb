{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "156454d8",
   "metadata": {},
   "source": [
    "## Performance Experiments with DeepSeek R1 Distill Qwen 7B Model\n",
    "\n",
    "This notebook performs performance experiments with the DeepSeek R1 Distill Qwen 7B model for inference. The aim is to evaluate how different parameters and configurations affect the model’s latency and resource usage. In the experiments, we will focus on:\n",
    "\n",
    "- Comparing inference times while varying parameters such as the number of maximum new tokens and truncation lengths.\n",
    "- Assessing how changes in memory allocation (using the max_memory variable) impact performance.\n",
    "- Exploring different precision and quantization settings via the BitsAndBytesConfig.\n",
    "- Measuring runtime with the current model configuration as a baseline.\n",
    "\n",
    "This workflow provides useful feedback on optimizing inference performance and resource management for large-scale language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "833d51b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.6.0+cu124\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch and CUDA compability\n",
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013674d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821466a",
   "metadata": {},
   "source": [
    "Using 4 bit BitsAndBytes quantization changes weights from 16 bit (2bytes/parameters) to 4 bit (0.5 byte/parameter) with some minute scale or metadata. For a 7B parameter model FP16 weights are approx 3GB where after 4bit it lands as 0.9 to 1.2 GB.\n",
    "\n",
    "It keeps only the weights as 4 bit where the Activations and Key Value cache of the Transformer model at FP16 to BF16 accuracy.\n",
    "\n",
    "4 bit is standard for memeory efficient QLoRA or LoRA finetuning for good inference in 8-12GB GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d50483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model for 4-bit quantization using BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c532840",
   "metadata": {},
   "source": [
    "1. (load_in_4bit=True): Quantizes model weights to 4 bit at load time. Activations, Layer Norms and Key_value cache are not quantized and stay in original compute type.\n",
    "2. (bnb_4bit_quant_type=\"nf4\"): NormalFloat4 preserve weights better than linear int4. nf4 is the best quality for doing 4 bit quantization\n",
    "3. (bnb_4bit_use_double_quant=True): Used in QLoRA for for storing weigthts in 4 bit along with per group scale/zero to reconstruct original float at runtime w=s(w1-z). s-> sca,e for group weights , z-> zero point. So these are even if metadata can add to the overhead. \n",
    "4. (bnb_4bit_compute_dtype=torch.float16): At matrix multiplication time 4 bit weights are dequantized into this datatype. Lowest VRAM usage and good for consumer cards. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ab3d5",
   "metadata": {},
   "source": [
    "device_map=\"auto\" is handled by Huggingface Accelerate. At load time it estimates each submodule size by following dtype/quantization then assigns whole blocks or layers to devices to be **fit within a budget** configured in max_memory. It fills GPU fast then spills rest of it in the CPU and remaining in disk if allowed. The result is static mapping not live rebalancing duing generation. Observed by model.hf_device_map\n",
    "\n",
    "sdpa attention is Pytorch's fast, memory-efficient scaled dot product attention.\n",
    "Using **low_cpu_mem_usage** streams weights during load to avoid big CPU/RAM spikes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b912df4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6577338c07430799c144f789ede172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define maximum memory allocation for CPU and GPU\n",
    "max_memory = {\n",
    "    0: \"5GiB\",\n",
    "    \"cpu\": \"25GiB\",\n",
    "}\n",
    "\n",
    "# Load tokenizer and model with specified configurations\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    torch_dtype=\"auto\",  # Automatically set to float16 or bfloat16 based on GPU for non-quantized models\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    quantization_config=bnb_config,\n",
    "    max_memory=max_memory,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14076b",
   "metadata": {},
   "source": [
    "- messages: the chat data to be sent to the model\n",
    "- apply_chat_template: formats these turns using model chat template then **tokenizes** it.\n",
    "- truncation, max_length: caps the prompt length to 512 tokens so KV-cache stays small as it its the main runtime VRAM hogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10d5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input messages for the model\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Imagine you are asked to consider, in the most detailed and precise way possible, how intelligence—whether it appears in human beings through biological processes shaped by evolution, or in artificial systems through training on data and optimization algorithms—can be rigorously defined, evaluated, and compared, and in doing so you must take into account the historical roots of the concept of intelligence from ancient philosophy through to the computational era, the biological foundations of cognition such as neurons, synapses, and brain networks, the computational equivalents such as artificial neurons, gradient descent, and representation learning, and then compare how metrics like accuracy or benchmark scores differ from broader qualities like adaptability, creativity, or robustness, and in this comparison also weigh the critiques such as Searle’s Chinese Room argument that challenge whether symbol manipulation equates to understanding, while considering whether embodied cognition, multimodal perception, or interaction with the real world might bring AI closer to human-like grounding, and then reflect on whether scaling laws alone can sustain progress or if new paradigms like neuromorphic hardware, probabilistic reasoning, or hybrid neuro-symbolic approaches are required, and finally I want you to integrate ethical, social, and cultural perspectives, since intelligence is not only a scientific matter but also one that influences human society, governance, and coexistence with AI, so the unifying question is: how can we construct a definition and measurement of intelligence that is at once scientifically rigorous, philosophically sound, practically useful, ethically responsible, and adaptable to future forms of cognition that may arise in both artificial and biological domains?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Prepare inputs using the tokenizer with chat template\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c8890",
   "metadata": {},
   "source": [
    "1. next(model.parameters()).device grabs the device of the first parameter shard\n",
    "2. Accelerate might have placed earlier layers on GPU 0 or cpu. Input must start where the first layers lives else there will be extra transfers or errors\n",
    "3. Then every tensor in inputs (input_ids, attention_mask) is moved to that device. So shapes/devices align with forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3fd6c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move inputs to the same device as the model\n",
    "device = next(model.parameters()).device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764db45",
   "metadata": {},
   "source": [
    "model.generate() calles dreedy decoder for causal LM to generate new tokens. Using KV cache so new tokens reuse previous attention states. Crucial for VRAM consumption.\n",
    "outputs a tensor of token_ids on the same device as model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba87aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I'm trying to wrap my head around this big question about what intelligence really is. The user wants a detailed and precise definition that applies to both humans and AI, considering everything from philosophy\n",
      "Time taken: 5.15 seconds\n"
     ]
    }
   ],
   "source": [
    "# Ensure CUDA operations are complete before starting timing\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Measure inference time\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# Generate outputs with a limit on new tokens\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1] :]))\n",
    "\n",
    "# Calculate and print time taken for inference\n",
    "dt = time.perf_counter() - t0\n",
    "print(f\"Time taken: {dt:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fbf200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move result off GPU, drop references, and flush caches\n",
    "outputs = outputs.to(\"cpu\")\n",
    "del inputs\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97a587ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First shard device: cuda:0\n",
      "Allocated MB: 5588.1\n",
      "Reserved  MB: 5653.9\n",
      "Device map: {'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"First shard device:\", next(model.parameters()).device)\n",
    "print(\"Allocated MB:\", round(torch.cuda.memory_allocated() / 1e6, 1))\n",
    "print(\"Reserved  MB:\", round(torch.cuda.memory_reserved() / 1e6, 1))\n",
    "print(\"Device map:\", model.hf_device_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
